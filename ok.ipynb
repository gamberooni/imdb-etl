{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9  ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "bf4611e601c174f023ee6d5ec1b694fc68b25a2a6a2cefb5ab3330490a5e6955"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "master = \"spark://zy-ubuntu:7077\"  \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = f'--master local[*] --driver-memory 6g --executor-memory 6g --packages org.postgresql:postgresql:42.1.1 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--master {master} --driver-memory 4g --total-executor-cores 6 --executor-memory 8g --packages org.postgresql:postgresql:42.1.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ok\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----+-----+---+\n|year|month|day|\n+----+-----+---+\n|2021|    6|  7|\n+----+-----+---+\n\n"
     ]
    }
   ],
   "source": [
    "# create a df with year, month, day with today's date\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "data = [(now.year,now.month,now.day)]\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"year\",IntegerType(),True), \\\n",
    "    StructField(\"month\",IntegerType(),True), \\\n",
    "    StructField(\"day\",IntegerType(),True), \\\n",
    "  ])\n",
    " \n",
    "dl_date_df = spark.createDataFrame(data=data,schema=schema)\n",
    "dl_date_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert dataset download date into db\n",
    "dl_date_df.write.format('jdbc').options(\n",
    "      url='jdbc:postgresql://localhost:5433/imdb',\n",
    "      driver='org.postgresql.Driver',\n",
    "      dbtable='dim_download_date',\n",
    "      user='admin',\n",
    "      password='password'\n",
    "      ).mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tsv file into df\n",
    "title_crew_df = spark.read.csv(\"title.crew.tsv\", sep=r'\\t', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the comma separated values in directors and writers column\n",
    "title_crew_df = title_crew_df.withColumn('directors', F.explode(F.split('directors', ',')))\n",
    "title_crew_df = title_crew_df.withColumn('writers', F.explode(F.split('writers', ',')))\n",
    "\n",
    "# create nconst and is_writer columns from writers column\n",
    "df_writers = title_crew_df.withColumn('nconst', F.when(F.col('writers') != '\\\\N', F.col(\"writers\"))) \\\n",
    "    .withColumn('is_writer', F.when(F.col('writers') != '\\\\N', F.lit(True)).otherwise(F.lit(False))).drop('writers') \\\n",
    "    .filter(F.col('nconst').isNotNull()).drop('directors')\n",
    "\n",
    "# create nconst and is_director columns from directors column\n",
    "df_directors = title_crew_df.withColumn('nconst', F.when(F.col('directors') != '\\\\N', F.col(\"directors\"))) \\\n",
    "    .withColumn('is_director', F.when(F.col('directors') != '\\\\N', F.lit(True)).otherwise(F.lit(False))).drop('directors') \\\n",
    "    .filter(F.col('nconst').isNotNull()).drop('writers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do full outer join \n",
    "df_joined = df_writers.join(df_directors, ['nconst','tconst'], how='full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dedupe after full outer join (maybe need to fix this)\n",
    "df_joined_deduped = df_joined.drop_duplicates(subset=['nconst', 'tconst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tsv file into df\n",
    "name_basics_df = spark.read.csv(\"name.basics.tsv\", sep=r'\\t', header=True)\n",
    "\n",
    "# rename column\n",
    "name_basics_df = name_basics_df.withColumnRenamed('primaryName', 'name')\n",
    "\n",
    "# calculate age from birth year and death year\n",
    "name_basics_df = name_basics_df.withColumn('age', ( F.col('deathYear').cast(IntegerType()) - F.col('birthYear').cast(IntegerType()) ))\n",
    "\n",
    "# create is_alive column based on conditions\n",
    "is_alive_col = F.when(\n",
    "    (F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") != '\\\\N'), False\n",
    ").when((F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") == '\\\\N'), True).otherwise(None)\n",
    "\n",
    "name_basics_df = name_basics_df.withColumn('is_alive', is_alive_col)\n",
    "\n",
    "# drop unused columns\n",
    "name_basics_df_dropped = name_basics_df.drop('primaryProfession', 'knownForTitles', 'birthYear', 'deathYear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join on the two processed dfs\n",
    "df_final = df_joined_deduped.join(name_basics_df_dropped, ['nconst'])\n",
    "\n",
    "# fill nulls with false \n",
    "df_final = df_final.fillna({'is_writer': False, 'is_director': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert df into dim_crew table\n",
    "df_final.write.format('jdbc').options(\n",
    "      url='jdbc:postgresql://localhost:5433/imdb',\n",
    "      driver='org.postgresql.Driver',\n",
    "      dbtable='dim_crew',\n",
    "      user='admin',\n",
    "      password='password'\n",
    "      ).mode('append').save()"
   ]
  }
 ]
}