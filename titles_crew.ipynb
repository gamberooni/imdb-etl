{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9  ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "bf4611e601c174f023ee6d5ec1b694fc68b25a2a6a2cefb5ab3330490a5e6955"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--master local[*] --driver-memory 6g --executor-memory 6g --packages org.postgresql:postgresql:42.1.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.id', 'local-1623000022957'),\n",
       " ('spark.sql.warehouse.dir', 'file:/home/zy/imdb-etl/spark-warehouse'),\n",
       " ('spark.app.startTime', '1623000022129'),\n",
       " ('spark.app.initial.file.urls',\n",
       "  'file:///home/zy/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar'),\n",
       " ('spark.driver.port', '36969'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://192.168.0.188:36969/jars/org.postgresql_postgresql-42.1.1.jar'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/home/zy/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar'),\n",
       " ('spark.driver.host', '192.168.0.188'),\n",
       " ('spark.jars',\n",
       "  'file:///home/zy/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///home/zy/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar'),\n",
       " ('spark.app.name', 'title crew'),\n",
       " ('spark.driver.memory', '6g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.files',\n",
       "  'file:///home/zy/.ivy2/jars/org.postgresql_postgresql-42.1.1.jar'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"title crew\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(tconst='tt0000001', directors='nm0005690', writers='\\\\N'),\n",
       " Row(tconst='tt0000002', directors='nm0721526', writers='\\\\N'),\n",
       " Row(tconst='tt0000003', directors='nm0721526', writers='\\\\N'),\n",
       " Row(tconst='tt0000004', directors='nm0721526', writers='\\\\N'),\n",
       " Row(tconst='tt0000005', directors='nm0005690', writers='\\\\N')]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "tsv_file = \"title.crew.tsv\"\n",
    "df1 = spark.read.csv(tsv_file, sep=r'\\t', header=True)\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- tconst: string (nullable = true)\n |-- directors: string (nullable = true)\n |-- writers: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "23350548"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Row(nconst='nm0000001', primaryName='Fred Astaire', birthYear='1899', deathYear='1987', primaryProfession='soundtrack,actor,miscellaneous', knownForTitles='tt0031983,tt0053137,tt0072308,tt0050419'),\n",
       " Row(nconst='nm0000002', primaryName='Lauren Bacall', birthYear='1924', deathYear='2014', primaryProfession='actress,soundtrack', knownForTitles='tt0037382,tt0038355,tt0117057,tt0071877'),\n",
       " Row(nconst='nm0000003', primaryName='Brigitte Bardot', birthYear='1934', deathYear='\\\\N', primaryProfession='actress,soundtrack,music_department', knownForTitles='tt0057345,tt0054452,tt0049189,tt0056404'),\n",
       " Row(nconst='nm0000004', primaryName='John Belushi', birthYear='1949', deathYear='1982', primaryProfession='actor,soundtrack,writer', knownForTitles='tt0080455,tt0072562,tt0077975,tt0078723'),\n",
       " Row(nconst='nm0000005', primaryName='Ingmar Bergman', birthYear='1918', deathYear='2007', primaryProfession='writer,director,actor', knownForTitles='tt0050976,tt0060827,tt0050986,tt0083922')]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "tsv_file2 = \"name.basics.tsv\"\n",
    "df2 = spark.read.csv(tsv_file2, sep=r'\\t', header=True)\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- nconst: string (nullable = true)\n |-- primaryName: string (nullable = true)\n |-- birthYear: string (nullable = true)\n |-- deathYear: string (nullable = true)\n |-- primaryProfession: string (nullable = true)\n |-- knownForTitles: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------+--------+-----+------+--------+-----+----------+---------------+\n|titleId|ordering|title|region|language|types|attributes|isOriginalTitle|\n+-------+--------+-----+------+--------+-----+----------+---------------+\n|      0|       0|    0|     1|       1|    1|         1|              1|\n+-------+--------+-----+------+--------+-----+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# show how many missing values for each column\n",
    "df.select(*(F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+--------+------------------------------+------+--------+-----+----------+---------------+\n|  titleId|ordering|                         title|region|language|types|attributes|isOriginalTitle|\n+---------+--------+------------------------------+------+--------+-----+----------+---------------+\n|tt3880980|       1|マックのハッスル刑事\tJP\tja\t...|  null|    null| null|      null|           null|\n+---------+--------+------------------------------+------+--------+-----+----------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.region.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "23350548"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df1 = df1.withColumn('directors', F.explode(F.split('directors', ',')))\n",
    "df1 = df1.withColumn('writers', F.explode(F.split('writers', ',')))\n",
    "\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_writers = df1.withColumn('nconst', F.when(F.col('writers') != '\\\\N', F.col(\"writers\"))) \\\n",
    "    .withColumn('is_writer', F.when(F.col('writers') != '\\\\N', F.lit(True)).otherwise(F.lit(False))).drop('writers') \\\n",
    "    .filter(F.col('nconst').isNotNull()).drop('directors')\n",
    "\n",
    "df1_directors = df1.withColumn('nconst', F.when(F.col('directors') != '\\\\N', F.col(\"directors\"))) \\\n",
    "    .withColumn('is_director', F.when(F.col('directors') != '\\\\N', F.lit(True)).otherwise(F.lit(False))).drop('directors') \\\n",
    "    .filter(F.col('nconst').isNotNull()).drop('writers')\n",
    "\n",
    "df1_writers.show()\n",
    "df1_directors.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19251658"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df1_writers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19497370"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df1_directors.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+---------+---------+-----------+\n|   nconst|   tconst|is_writer|is_director|\n+---------+---------+---------+-----------+\n|nm0000024|tt0926561|     true|       null|\n|nm0000033|tt7700220|     true|       null|\n|nm0000054|tt5149714|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n|nm0000092|tt0233023|     true|       null|\n+---------+---------+---------+-----------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# df1_writers.join(df1_directors, (df1_writers.nconst == df1_directors.nconst) & (df1_writers.tconst == df1_directors.tconst)).select('df1_writers.tconst','df1_directors.is_director').show(5)\n",
    "df_joined = df1_writers.join(df1_directors, ['nconst','tconst'], how='full')\n",
    "df_joined.show()\n",
    "\n",
    "# then replace nulls with false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "334327527"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df_joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13799810"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df_joined_deduped = df_joined.drop_duplicates(subset=['nconst', 'tconst'])\n",
    "df_joined_deduped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o254.showString.\n: org.apache.spark.SparkException: Job 13 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2154)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2404)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-43fe08f078f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_joined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/imdb-etl/venv/lib/python3.6/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/imdb-etl/venv/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/imdb-etl/venv/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/imdb-etl/venv/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o254.showString.\n: org.apache.spark.SparkException: Job 13 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2154)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2404)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "df_joined_deduped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+-------------------+----+--------+\n|   nconst|               name| age|is_alive|\n+---------+-------------------+----+--------+\n|nm0000001|       Fred Astaire|  88|   false|\n|nm0000002|      Lauren Bacall|  90|   false|\n|nm0000003|    Brigitte Bardot|null|    true|\n|nm0000004|       John Belushi|  33|   false|\n|nm0000005|     Ingmar Bergman|  89|   false|\n|nm0000006|     Ingrid Bergman|  67|   false|\n|nm0000007|    Humphrey Bogart|  58|   false|\n|nm0000008|      Marlon Brando|  80|   false|\n|nm0000009|     Richard Burton|  59|   false|\n|nm0000010|       James Cagney|  87|   false|\n|nm0000011|        Gary Cooper|  60|   false|\n|nm0000012|        Bette Davis|  81|   false|\n|nm0000013|          Doris Day|  97|   false|\n|nm0000014|Olivia de Havilland| 104|   false|\n|nm0000015|         James Dean|  24|   false|\n|nm0000016|    Georges Delerue|  67|   false|\n|nm0000017|   Marlene Dietrich|  91|   false|\n|nm0000018|       Kirk Douglas| 104|   false|\n|nm0000019|   Federico Fellini|  73|   false|\n|nm0000020|        Henry Fonda|  77|   false|\n+---------+-------------------+----+--------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumnRenamed('primaryName', 'name')\n",
    "df2 = df2.withColumn('age', ( df2['deathYear'].cast(IntegerType()) - df2['birthYear'].cast(IntegerType()) ))\n",
    "\n",
    "is_alive_col = F.when(\n",
    "    (F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") != '\\\\N'), False\n",
    ").when((F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") == '\\\\N'), True).otherwise(None)\n",
    "\n",
    "df2 = df2.withColumn('is_alive', is_alive_col)\n",
    "\n",
    "df2 = df2.drop('primaryProfession', 'knownForTitles', 'birthYear', 'deathYear')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------+---------+---------+--------------+----+--------+\n|    tconst|  writers|   nconst|          name| age|is_alive|\n+----------+---------+---------+--------------+----+--------+\n| tt0078813|nm0596942|nm0000086|Louis de Funès|  69|   false|\n| tt0078813|nm0000086|nm0000086|Louis de Funès|  69|   false|\n| tt0078813|nm0320833|nm0000086|Louis de Funès|  69|   false|\n| tt0119792|nm0000198|nm0000198|   Gary Oldman|null|    true|\n|tt14056430|       \\N|nm0000198|   Gary Oldman|null|    true|\n+----------+---------+---------+--------------+----+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_final = df_joined_deduped.join(df2, ['nconst'])\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}