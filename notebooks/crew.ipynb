{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9  ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "bf4611e601c174f023ee6d5ec1b694fc68b25a2a6a2cefb5ab3330490a5e6955"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.types import IntegerType\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "master = \"spark://zy-ubuntu:7077\"  \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--master {master} --driver-memory 4g --total-executor-cores 8 --executor-memory 8g --packages org.postgresql:postgresql:42.1.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"crew\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# set config to read from minio\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"admin\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"password\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://192.168.0.188:9000\")  # must use IP address\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"-Dcom.amazonaws.services.s3.enableV4\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.multipart.size\", \"104857600\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- tconst: string (nullable = true)\n |-- ordering: string (nullable = true)\n |-- nconst: string (nullable = true)\n |-- category: string (nullable = true)\n |-- job: string (nullable = true)\n |-- characters: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "title_principals_df = spark.read.csv(\"s3a://imdb/2021-06/06/title.principals.tsv\", sep=r'\\t', header=True)  \n",
    "title_principals_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_crew_df = title_principals_df.filter(\n",
    "        (F.col('category') != 'self') &\n",
    "        (F.col('category') != 'actor') & \n",
    "        (F.col('category') != 'actress'))\n",
    "\n",
    "title_crew_df = title_crew_df.drop('job').drop('characters').drop('ordering')\n",
    "title_crew_df = title_crew_df.withColumnRenamed('category', 'role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tsv file into df\n",
    "name_basics_df = spark.read.csv(\"s3a://imdb/2021-06/06/name.basics.tsv\", sep=r'\\t', header=True)  \n",
    "\n",
    "# rename column\n",
    "name_basics_df = name_basics_df.withColumnRenamed('primaryName', 'name')\n",
    "\n",
    "# calculate age from birth year and death year - there will be a scenario where birth year is known but death year is not (there are some ppl a few hundreds y/o)\n",
    "name_basics_df = name_basics_df.withColumn(\"age\", when((F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") != '\\\\N'), (F.col('deathYear').cast(IntegerType()) - F.col('birthYear').cast(IntegerType()))).when((F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") == '\\\\N'), (datetime.datetime.now().year - F.col('birthYear')).cast(IntegerType())).otherwise(None))\n",
    "\n",
    "# create is_alive column based on conditions\n",
    "is_alive_col = F.when(\n",
    "    (F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") != '\\\\N'), False\n",
    ").when((F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") == '\\\\N'), True).otherwise(None)\n",
    "\n",
    "name_basics_df = name_basics_df.withColumn('is_alive', is_alive_col)\n",
    "\n",
    "# drop unused columns\n",
    "name_basics_df_dropped = name_basics_df.drop('primaryProfession', 'knownForTitles', 'birthYear', 'deathYear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crew = title_crew_df.join(name_basics_df_dropped, ['nconst'])\n",
    "# df_crew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upload = df_crew.drop('tconst').drop('role').drop_duplicates(['nconst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upload.write.format('jdbc').options(\n",
    "      url='jdbc:postgresql://localhost:5433/imdb',\n",
    "      driver='org.postgresql.Driver',\n",
    "      dbtable='crew',\n",
    "      user='admin',\n",
    "      password='password'\n",
    "      ).mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_df = spark.read.format('jdbc').options(\n",
    "      url='jdbc:postgresql://localhost:5433/imdb',\n",
    "      driver='org.postgresql.Driver',\n",
    "      dbtable='titles',\n",
    "      user='admin',\n",
    "      password='password'\n",
    "      ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_id_df = titles_df.select('id', 'tconst').withColumnRenamed('id', 'title_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: short (nullable = true)\n |-- is_alive: boolean (nullable = true)\n |-- nconst: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "crew_from_pg_df = spark.read.format('jdbc').options(\n",
    "      url='jdbc:postgresql://localhost:5433/imdb',\n",
    "      driver='org.postgresql.Driver',\n",
    "      dbtable='crew',\n",
    "      user='admin',\n",
    "      password='password'\n",
    "      ).load()\n",
    "crew_from_pg_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- nconst: string (nullable = true)\n |-- tconst: string (nullable = true)\n |-- role: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- is_alive: boolean (nullable = true)\n\nroot\n |-- title_id: integer (nullable = true)\n |-- tconst: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_crew.printSchema()\n",
    "titles_id_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_id_df = crew_from_pg_df.select('id', 'nconst').withColumnRenamed('id', 'crew_id')\n",
    "df_crew_composite_table = df_crew.select('nconst', 'tconst', 'role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = crew_id_df.join(df_crew_composite_table, ['nconst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles_crew = df_tmp.join(titles_id_df, ['tconst'])\n",
    "df_titles_crew = df_titles_crew.drop('nconst').drop('tconst')\n",
    "# df_titles_crew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles_crew.write.format('jdbc').options(\n",
    "      url='jdbc:postgresql://localhost:5433/imdb',\n",
    "      driver='org.postgresql.Driver',\n",
    "      dbtable='titles_crew',\n",
    "      user='admin',\n",
    "      password='password'\n",
    "      ).mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}