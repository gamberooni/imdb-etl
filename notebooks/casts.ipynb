{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "master = \"spark://zy-ubuntu:7077\"  \n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--master {master} --driver-memory 4g --total-executor-cores 8 --executor-memory 8g --packages org.postgresql:postgresql:42.1.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"casts\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# set config to read from minio\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"admin\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"password\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://192.168.0.188:9000\")  # must use IP address\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"-Dcom.amazonaws.services.s3.enableV4\", \"true\")\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.multipart.size\", \"104857600\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- tconst: string (nullable = true)\n |-- ordering: string (nullable = true)\n |-- nconst: string (nullable = true)\n |-- category: string (nullable = true)\n |-- job: string (nullable = true)\n |-- characters: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "title_principals_df = spark.read.csv(\"s3a://imdb/2021-06/06/title.principals.tsv\", sep=r'\\t', header=True)  \n",
    "title_principals_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_principals_df = title_principals_df.drop('ordering').drop('job').drop('category')\n",
    "\n",
    "title_principals_df = title_principals_df.withColumn(\"characters\", F.regexp_replace(F.col(\"characters\"), '[\\[\\]\\\"]', \"\").alias(\"replaced\"))\n",
    "title_principals_df = title_principals_df.withColumn('characters', F.explode(F.split('characters', ',')))\n",
    "title_principals_df = title_principals_df.withColumnRenamed('characters', 'character')\n",
    "title_casts_df = title_principals_df.filter(\n",
    "        (F.col('category') == 'self') | \n",
    "        (F.col('category') == 'actor') | \n",
    "        (F.col('category') == 'actress'))\n",
    "# title_casts_df.show(truncate=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read tsv file into df\n",
    "name_basics_df = spark.read.csv(\"s3a://imdb/2021-06/06/name.basics.tsv\", sep=r'\\t', header=True)  \n",
    "\n",
    "# rename column\n",
    "name_basics_df = name_basics_df.withColumnRenamed('primaryName', 'name')\n",
    "\n",
    "# calculate age from birth year and death year\n",
    "name_basics_df = name_basics_df.withColumn(\"age\", when((F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") != '\\\\N'), (F.col('deathYear').cast(IntegerType()) - F.col('birthYear').cast(IntegerType()))).when((F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") == '\\\\N'), (datetime.datetime.now().year - F.col('birthYear')).cast(IntegerType())).otherwise(None))\n",
    "\n",
    "# create is_alive column based on conditions\n",
    "is_alive_col = F.when(\n",
    "    (F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") != '\\\\N'), False\n",
    ").when((F.col(\"birthYear\") != '\\\\N') & (F.col(\"deathYear\") == '\\\\N'), True).otherwise(None)\n",
    "\n",
    "name_basics_df = name_basics_df.withColumn('is_alive', is_alive_col)\n",
    "\n",
    "# drop unused columns\n",
    "name_basics_df_dropped = name_basics_df.drop('primaryProfession', 'knownForTitles', 'birthYear', 'deathYear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_casts = title_casts_df.join(name_basics_df_dropped, ['nconst'])\n",
    "df_casts = df_casts.withColumn('character', F.when(F.col('character') == '\\\\N', F.lit(None)).otherwise(F.col('character')))\n",
    "\n",
    "# df_casts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- nconst: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- is_alive: boolean (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_upload = df_casts.drop('tconst').drop('character').drop_duplicates(['nconst'])\n",
    "# df_upload.show()\n",
    "df_upload.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert df into dim_casts table\n",
    "df_upload.write.format('jdbc').options(\n",
    "      url='jdbc:postgresql://localhost:5433/imdb',\n",
    "      driver='org.postgresql.Driver',\n",
    "      dbtable='casts',\n",
    "      user='admin',\n",
    "      password='password'\n",
    "      ).mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- download_date_id: integer (nullable = true)\n |-- tconst: string (nullable = true)\n |-- type: string (nullable = true)\n |-- primary_title: string (nullable = true)\n |-- original_title: string (nullable = true)\n |-- is_adult: boolean (nullable = true)\n |-- start_year: short (nullable = true)\n |-- end_year: short (nullable = true)\n |-- runtime_minutes: short (nullable = true)\n |-- av_rating: float (nullable = true)\n |-- num_votes: integer (nullable = true)\n |-- genre_1: string (nullable = true)\n |-- genre_2: string (nullable = true)\n |-- genre_3: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "titles_df = spark.read.format('jdbc').options(\n",
    "      url='jdbc:postgresql://localhost:5433/imdb',\n",
    "      driver='org.postgresql.Driver',\n",
    "      dbtable='titles',\n",
    "      user='admin',\n",
    "      password='password'\n",
    "      ).load()\n",
    "titles_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_id_df = titles_df.select('id', 'tconst')\n",
    "titles_id_df = titles_id_df.withColumnRenamed('id', 'title_id')\n",
    "# title_desc_id_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- age: short (nullable = true)\n |-- is_alive: boolean (nullable = true)\n |-- nconst: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "casts_from_pg_df = spark.read.format('jdbc').options(\n",
    "      url='jdbc:postgresql://localhost:5433/imdb',\n",
    "      driver='org.postgresql.Driver',\n",
    "      dbtable='casts',\n",
    "      user='admin',\n",
    "      password='password'\n",
    "      ).load()\n",
    "casts_from_pg_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- nconst: string (nullable = true)\n |-- tconst: string (nullable = true)\n |-- character: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- is_alive: boolean (nullable = true)\n\nroot\n |-- title_id: integer (nullable = true)\n |-- tconst: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_casts.printSchema()\n",
    "titles_id_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "casts_id_df = casts_from_pg_df.select('id', 'nconst')\n",
    "casts_id_df = casts_id_df.withColumnRenamed('id', 'cast_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_casts_composite_table = df_casts.select('nconst', 'tconst', 'character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = casts_id_df.join(df_casts_composite_table, ['nconst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- cast_id: integer (nullable = true)\n |-- character: string (nullable = true)\n |-- title_id: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_titles_casts = df_tmp.join(titles_id_df, ['tconst'])\n",
    "df_titles_casts = df_titles_casts.drop('nconst').drop('tconst')\n",
    "df_titles_casts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert df into titles_casts table\n",
    "df_titles_casts.write.format('jdbc').options(\n",
    "      url='jdbc:postgresql://localhost:5433/imdb',\n",
    "      driver='org.postgresql.Driver',\n",
    "      dbtable='titles_casts',\n",
    "      user='admin',\n",
    "      password='password'\n",
    "      ).mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}